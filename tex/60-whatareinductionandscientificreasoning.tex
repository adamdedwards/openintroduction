\chapter{What are Induction and Scientific Reasoning?}
\markright{Chap \ref{ch:inductionandscience}: Induction and Science}
\label{ch:inductionandscience}
\setlength{\parindent}{1em}

\section{Introduction}

Earlier in our discussion of logic we made the distinction between deductive and inductive forms of inference. Recall that while deduction and deductive inferences are inferences that attempt to guarantee the truth of the conclusion on the basis of the premises, induction and inductive inferences attempt something less ambitious. In short, induction is any form of inference in which the premises are not intended to logically entail or guarantee their conclusion, but merely to provide some support to it. In this part of the book we'll be examining inductive logic in more detail, with special attention paid to a rigorous form of inductive reasoning as applied in the sciences.

Before we begin, you may be asking yourself, ``Why should we care about inductive reasoning at all?'' After all, deductive inferences \emph{guarantee} their conclusions and that's the best kind of evidential support you could hope for! Shouldn't we just spend all of our time focusing on making deductive inferences?

This is a reasonable complaint, and one worth taking seriously. While valid deduction is the absolute best form of evidential support a conclusion can receive, we should care about induction as well, for at least the following reasons.

First, many of the inferences people make every day are inductive, not deductive. Consider just the inferences you make when you go to the refrigerator to make a sandwich. Suppose you reason as follows:

\begin{kormanize}
\premise{Yesterday I bought sandwich ingredients and put them in my refrigerator.}
\conclusion{So, today there are sandwich ingredients in my refrigerator.}
\end{kormanize}

Of course, the conclusion isn't entailed by that premise. Perhaps your roommate ate all of the cold cuts, cheese, and vegetables that you bought at the store yesterday in a late night binge. If so, then the conclusion obviously would not follow from the premise. Okay, but we can deal with this by adding another premise, such as ``No one has touched my sandwich ingredients since I put them in the refrigerator.'' Still, the conclusion does not follow \emph{deductively} from these premises since it could be that while no one has touched them the power went out in your building and all of the food in your refrigerator has gone bad!

We could go on and on like this, but the point is that most of the inferences we make are not in the form of a deductively valid argument. We often reason on limited evidence, or via less than valid forms of inference. So it is worthwhile to understand how inductive inference functions.

A second reason to care about inductive inference is that deductive inferences are fundamentally \emph{limited}. By this I mean to emphasize the logician and philosopher C. S. Peirce's term for inductive inference: \textsc{\gls{ampliative logic}}. By ampliative, Peirce meant that the premises `amplify' what can be deductively inferred from them. That is, the premises go beyond their deductive entailments.

\newglossaryentry{ampliative logic}
{
name=Ampliative Logic,
description={Ampliative logic is another term for inductive logic, or a logic the inferences of which do not guarantee or logically entail the truth of the conclusion on the basis of the premises. See also \gls{inductive logic}.}
}

Ampliative inference is just another word for inductive inference, but the point is the same. Inductive inferences tell us something about the world that is not already contained in the premises. For example, consider this classic deductive argument from categorical logic:

\begin{kormanize}
\premise{All dogs like peanut butter.}
\premise{Rose is a dog.}
\conclusion{So, Rose likes peanut butter.}
\end{kormanize}

Now consider that if you \emph{accept} the premises, then there's an important sense in which the conclusion doesn't tell you anything you didn't already know. You haven't learned anything about the world from this argument (assuming you already knew the premises). However, contrast that argument with the following inductive `version': 

\begin{kormanize}
\premise{Most dogs like peanut butter.}
\premise{Rose is a dog.}
\conclusion{So, Rose likes peanut butter.}
\end{kormanize}

Obviously, the conclusion isn't entailed by these premises (the argument isn't valid). However, if you come to know the conclusion on the basis of these premise you \emph{do} learn something new; namely, you learn that Rose is one of the majority of dogs who like peanut butter. 

A final consideration for why induction is important is simply that most scientific inferences are made as inductive, rather than deductive, inferences. The logic of science and statistics is fundamentally inductive, not deductive. To gain a better understanding of science it is extremely important to understand how inductive inference functions.

Before moving on to the next chapters, we'll spend the remainder of this chapter discussing some basic concepts in the history and philosophy of science in order to give some context for later discussions.

\section{Early Scientific Method}

The history of philosophy is the history of science. Aristotle wrote lengthy treatises on the nature of mechanics and dynamics, on biology, geology, and optics. For most of history there was no sharp distinction between the scientist (a term invented by the philosopher and scientist William Whewell in 1833) and the philosopher.

One of the things that characterizes the early development of the scientific method is a commitment to \textsc{\gls{empiricism}}. Empiricism is the view that the only, or perhaps the primary, source of knowledge is sensory evidence or the experience of the senses. This view is contrasted with \textsc{\gls{rationalism}}, which says that the source of knowledge is the internal mechanisms of reason.

\newglossaryentry{empiricism}
{
name=Empiricism,
description={Empiricism is a view in epistemology according to which the only, or perhaps the primary, source of knowledge is sensory evidence or the experience of the senses. See also \gls{rationalism}.}
}

\newglossaryentry{rationalism}
{
name=Rationalism,
description={Rationalism is the view in epistemology that knowledge comes from internal mechanisms of reason or rationality. See also \gls{empiricism}.}
}

Both of these views have had their philosophical defenders and both have a lot of evidence that we can acquire knowledge in the way recommended by the view. Consider, for example, proofs in geometry. The truths of mathematics in general, and geometry in particular, seem to be true no matter what. That is, these truths don't depend on making certain kinds of observations or collecting any evidence. Thus, the rationalists say, truths of this kind of derivable from pure reason. Notice that even though you and I may have made observations of a certain kind in our geometry class in order to come to know these truths, this doesn't undermine the rationalist position that such observations are not \emph{required} in order to know geometric proofs. Strict empiricists, on the other hand, maintain that mathematical \emph{concepts} require some kind of of observation and that a being of pure reason and no senses (if such a thing is possible) would not be able to create these concepts from nothing. 

Which of these views do you find more compelling? Think about where knowledge might come from and what kinds of powers are necessary in order to know things about the world. We'll focus on empiricism for now, but think about the truths of logic and their source. Are these truths the result of sensory observations or reason?

Early empiricists noticed very quickly that there are better and worse ways to observe the world around us. As a result they attempted to develop \emph{methods} that would codify and regulate the best ways to make empirical observations and make inferences on the basis of those observations. This is an ongoing process.

There is no one ``scientific method'' that describes the best way to observe, predict, and model the world. Philosophers of science have been engaged in the centuries-long development of various scientific methods to help us make better inferences. 

Starting with Sir Francis Bacon's \textit{Novum Organum} and Rene Descartes' \textit{Discourse on the Method} and ' \textit{Principles of Philosophy} in the 17th century, philosophers of science have generally agreed that a central feature of science is the development of a \textsc{\gls{hypothesis}}. A hypothesis is simply a proposition that may or may not be true and that is subjected to some form of confirmation or disconfirmation.


\newglossaryentry{hypothesis}
{
name=Hypothesis,
description={A hypothesis is a proposition that may or may not be true and can be subjected to confirmation or disconfirmation.}
}


Bacon's method was to begin with the simplest and most obvious observations and then use these observations to confirm or disconfirm hypotheses at greater levels of generality. Descartes, on the other hand, began by doubting the truth of everything that his senses presented to him and trying to identify hypotheses that could not possibly be doubted (such as the famous \emph{cogito ergo sum}, or ``I think, therefore I am.''). Descartes then used this foundation of indubitable truths to derive more commonplace beliefs.

\section{The Hypothetico-Deductive Model}

The \textsc{\gls{hypothetico-deductive model}} of scientific inference is a proposed logical form of inference that explains how scientific hypotheses can be confirmed or disconfirmed.

\newglossaryentry{hypothetico-deductive model}
{
name=Hypothetico-Deductive Model,
description={}
}

The hypothetico-deductive model is one of the more basic methods common to all scientific disciplines, whether it is economics, physics, or biochemistry. Its application can be divided into four stages:

\begin{enumerate}
\item Identify the hypothesis to be tested.
\item Generate predictions from the hypothesis.
\item Perform experiments to check whether predictions are correct.
\item If the predictions are correct, then the hypothesis is confirmed. Otherwise, the hypothesis is disconfirmed.
\end{enumerate}

Suppose your portable music player fails to switch on. You might then consider the hypothesis that perhaps the batteries are dead. So you decide to test whether this is true. Given this hypothesis you predict that the music player should work properly if you replace the batteries with new ones. So you proceed to replace the batteries, which is the ``experiment'' for testing the prediction. If the player works again, then your hypothesis is confirmed, and so you throw away the old batteries. If the player still does not work, then the prediction is false, and the hypothesis is disconfirmed. So you might reject your original hypothesis and come up with an alternative one to test, e.g. the batteries are ok but your music player is broken.

The example above helps us illustrate a few points about science and the hypothetico-deductive method.

\begin{enumerate}
\item A scientific hypothesis must be testable.
\item Confirmation is not truth.
\item Disconfirmation need not be falsity.
\end{enumerate}

Let's consider these points one by one.

First, notice that for us to apply the hypothetico-deductive method we must be able to test our hypothesis. The hypothetico-deductive method tells us that our hypothesis must be capable of being tested, but the method doesn't tell us (a) how to test our hypothesis or (b) how to distinguish between a testable or untestable hypothesis.

If a hypothesis cannot be tested, we cannot find evidence to show that it is probable or not. In that case it cannot be part of scientific knowledge. Consider the hypothesis that there are ghosts that have no causal efficacy, that we cannot neither see nor interact with, and which can never be detected directly or indirectly. This hypothesis is defined in such a way to exclude the possibility of being tested. It might still be true and there might be such ghosts, but we would never be in a position to know and so this cannot be a scientific hypothesis.

Second, notice that confirming the predictions of a hypothesis increases the probability that a hypothesis is correct. But in itself this does not prove conclusively that the hypothesis is correct.

To see why this is the case, we might represent our reasoning as follows:

\begin{kormanize}
\premise{If H then P.}
\premise{P.}
\conclusion{Therefore H.}
\end{kormanize}

\newglossaryentry{process of confirmation}
{
name=Process of Confirmation,
description={}
}

Here H is our hypothesis ``the batteries are dead", and P is the prediction ``the player will function when the batteries are replaced.'' This pattern of reasoning is of course not valid, since there might be reasons other than H that also bring about the truth of P. For example, it might be that the original batteries are actually fine, but they were not inserted properly. Replacing the batteries would then restore the loose connection. So the fact that the prediction is true does not prove that the hypothesis is true. We need to consider alternative hypotheses and see which is more likely to be true and which provides the best explanation of the prediction. (Or we can also do more testing!)

Finally, consider that disconfirmation is sometimes not enough to demonstrate the falsity of the hypothesis. Very often a hypothesis generates a prediction only when given additional assumptions (auxiliary hypotheses). In such cases, when a prediction fails the hypothesis might still be correct.

Looking back at our example again, when we predict that the player will work again when the batteries are replaced, we are assuming that there is nothing wrong with the player. But it might turn out that this assumption is wrong. In such situations the falsity of the prediction does not logically entail the falsity of the hypothesis. We might depict the situation by this argument : ( H=The batteries are dead, A=The player is not broken.)

\begin{kormanize}
\premise{If both H and A, then P.}
\premise{It is not the case that P.}
\conclusion{Therefore, it is not the case that H.}
\end{kormanize}

This argument is of course not valid. When P is false, what follows is not that H is false, only that the conjunction of H and A is false. So there are three possibilities : (a) H is false but A is true, (b) H is true but A is false, or (c) both H and A are false. So we should argue instead :

\begin{kormanize}
\premise{If both H and A, then P.}
\premise{It is not the case that P.}
\conclusion{Therefore, it is not the case that both H and A are true.}
\end{kormanize}

Returning to our earlier example, if the player still does not work when the batteries are replaced, this does not prove conclusively that the original batteries are not dead. This tells us that when we apply the hypothetico-deductive method, we need to examine the additional assumptions that are invoked when deriving the predictions. If we are confident that the assumptions are correct, then the falsity of the prediction would be a good reason to reject the hypothesis. On the other hand, if the hypothesis we are testing has been extremely successful, then we need to be extremely cautious before we reject a hypothesis on the basis of a single false prediction. These additional assumptions used in testing a hypothesis are known as ``auxiliary hypotheses.''

\newglossaryentry{process of disconfirmation}
{
name=Process of Disconfirmation,
description={}
}

\section{The Demarcation Criterion}

How do we distinguish between a scientific and non-scientific hypothesis? Since the philosopher of science Karl Popper, philosophers have referred to the quality or property that distinguishes between science and non-science as the \textsc{\gls{demarcation criterion}}. According to Popper, the demarcation criterion was that a scientific hypothesis must be \textsc{\gls{falsifiable}}, whereas a pseudoscientific or non-scientific hypothesis was not falsifiable.

\newglossaryentry{demarcation criterion}
{
name=Demarcation Criterion,
description={The demarcation criterion is the feature of a hypothesis that distinguishes it as scientific or non-scientific.}
}

\newglossaryentry{falsifiable}
{
name=Falsifiable,
description={A hypothesis is falsifiable if there is some evidence or observation that would demonstrate that the hypothesis is false.}
}

An example of a falsifiable hypothesis is hypothesis that \text{the Earth is flat}. This hypothesis is falsifiable because it can be subjected to several tests such as measuring the angle of approaching ships, shadows, the position of the moon and its path in the sky in different parts of the Earth, and by launching a rocket and making a direct observation that the Earth is not flat. That the Earth is round is likewise falsifiable because it can be falsified by subjecting it to similar tests. The difference between these hypotheses isn't that one is ``more'' scientific than the other; they are equally scientific according to Popper's criterion.

However, there is one important difference between them: one is false and the other is true! The Earth is round and not flat, and we've subjected these hypotheses to many, many tests that have successfully falsified the flat Earth hypothesis and failed to falsify the round Earth hypothesis.\footnote{The epistemic and logical features are an interesting and growing area of philosophical and psychological research.}

An example of an unfalsifiable hypothesis would be the hypothesis that reading Tarot cards will give you insight into your true purpose in life. You can't know that the hypothesis is \emph{false}, since it is vague and relies on a specific interpretation and value system. Another non-falsifiable hypothesis is that God exists. This hypothesis is consistent with any observation, since if the hypothesis is true then it can be used to explain any outcome.

That these hypotheses are non-scientific doesn't mean they aren't meaningful, interesting, or worthwhile. They may also be true or false. What the demarcation criterion tells us is that these hypotheses aren't scientific or amenable to scientific testing. Many ethical, aesthetic, political, theological, philosophical and moral questions are non-scientific.

An important distinction, and the one that Popper initially set out to establish, is between scientific hypotheses and \textsc{\gls{pseudoscience}}. Pseudoscientific hypotheses are not falsifiable---so, according to Popper, they are non-scientific---but they \emph{claim to be} scientific by adopting certain practices or rhetorical moves that scientists adopt. Typically, a perfectly fine scientific hypothesis---such as the flat Earth hypothesis---will become pseudoscientific when it is not rejected appropriately. By adopting a scientific stance, its defenders can claim a kind of epistemic or logical authority that they do not have. 

For example, it is pseudoscientific to defend the flat Earth hypothesis for seemingly scientific reasons while refusing to acknowledge the enormous body of evidence that the hypothesis is false. Alternatively pseudoscientific hypotheses are defended by conducting flawed experiments that do not establish what they purport to establish. Another example of a pseudoscientific hypothesis is young Earth creationism, which explains the existence of fossils and geological evidence for the age of the Earth by appeal to God or another non-falsifiable hypothesis. 

\newglossaryentry{pseudoscience}
{
name=Pseudoscience,
description={A theory or hypothesis is pseudoscientific when it fails to meet the demarcation criterion but adopts rhetorical or argumentative practices that appear to be scientific.}
}

One of the central philosophical challenges is that it is often unclear from the evidence available whether we should consider an ordinary scientific experiment to have falsified a hypothesis outright, or whether we should reject a different, auxiliary hypothesis. This is something that defenders of pseudoscience will often exploit. For example, antivaccination defenders will explain away the numerous falsifications of their hypothesis that vaccinations cause autism or are laced with deadly toxins by claiming that these studies do not replicate. In fact, replication of experimental results is a serious concern in scientific research and so their concern \emph{seems} scientific. However, the studies that reject antivaccination have been extremely well established and replicated\footnote{See e.g. Taylor et. al 2014 (\href{https://www.ncbi.nlm.nih.gov/pubmed/24814559}{link}) for a recent meta-analysis of this research.} so this concern is unwarranted. But knowing when we should reject a hypothesis is a topic of ongoing debate in the philosophy of science.

\section{When should we reject a hypothesis?}

When a hypothesis makes a false prediction, sometimes it can be difficult to know whether we should reject the hypothesis or whether there is something wrong with the auxiliary hypotheses. For example, astronomers in the 19th century found that Newtonian physics could not fully explain planet Mercury's orbit. It turns out that this is because Newtonian physics makes some incorrect assumptions about the structure of space and time, and you need relativity to give a more accurate prediction of the orbit. However, when astronomers discovered Uranus in 1781, they also found out that its orbit was different from the predictions of Newtonian physics. But then scientists realized that it could be explained if there was an additional planet which affected Uranus, and Neptune was subsequently discovered as a result.

Thus, in the case of Mercury's orbit the theory of Newtonian mechanics was falsified and the behavior of the orbit was only explained once we developed relativity theory. However, in the case of Uranus's orbit an auxiliary hypothesis (that another planet existed in the vicinity of Uranus) explained the deviation from the prediction of Newtonian mechanics. 

In 2011, scientists in Italy reported that their experiment seemed to have shown that some subatomic particles could travel faster than the speed of light, which would seem to show that relativity is wrong. But on closer inspection, it was discovered that there was a problem with the experimental setup. So if a hypothesis has been very successful, even when a result seems to show that hypothesis is wrong, we need to make sure that the evidence is strong and reliable, and try to replicate the result and eliminate alternative explanations.

\section{Causation}

Causal relationships are an extremely important part of what we know. It matters to us that our actions are efficacious. If you take medicine for an illness, it matters whether the medicine \emph{cured} your illness or that you just happened to get better.

Philosophers of science have been interested in causation since Aristotle. Aristotle defined four kinds of causation: effective causation, material causation, formal causation, and final causation. The kind of causation that most people have in mind nowadays is what Aristotle called effective causation. That is, the effective cause of an object like a chair is the series of actions taken by a machine or carpenter to construct the chair.

By material cause, Aristotle meant the substance out of which the object is made. So, in our chair example the material cause would be the wood, nails, fabric, and so on. This is contrasted for Aristotle with the formal cause, which is the abstract form of the object or its design. Many different chairs may share a formal cause while differing with respect to their material causes.

Finally, there is the final cause of an object. The final cause of something is the purpose or function that it was built to perform. For our chair, presumably, the final cause would be for someone to sit on.

Our philosophical and scientific understanding of causation has come a long way since Aristotle. Now we recognize that there are many conditions that need to be met in order to establish a causal hypothesis. One advancement over Aristotle's discussion of causation is the distinction between necessary and sufficient conditions.

A \textsc{\gls{necessary condition}} for something is a state of affairs that \emph{must} obtain in order for the effect to occur. For example, oxygen is a necessary condition for fire. The basic idea is that a necessary condition is something that is needed in order for the cause to have its expected effect. Necessary conditions are often treated as `background conditions' because they aren't enough on their own to bring about an effect but they are, well, necessary.

A \textsc{\gls{sufficient condition}} is a state of affairs that is enough on its own for an effect to come about. For example, caffeine withdrawal is a sufficient condition for a headache. A sufficient condition is often treated like the `active' cause of some effect because, given appropriate background conditions, a sufficient condition obtaining will bring about an effect.

\newglossaryentry{necessary condition}
{
name=necessary condition,
description={A necessary condition is a state of affairs that must obtain in order for an effect to come about. For example, oxygen is a necessary condition for fire.}
}

\newglossaryentry{sufficient condition}
{
name=sufficient condition,
description={A sufficient condition is a state of affairs that is enough on its own for an effect to come about. For example, caffeine withdrawal is a sufficient condition for a headache.}
}

Generally speaking, we are interested in both necessary and sufficient conditions for some effect to understand how the effect is brought about. However, we often struggle to identify the precise necessary and sufficient conditions to bring about all the effects we care about understanding. For this reason we will try to identify when two properties or states of affairs are associated.

Consider whether two properties are associated in a system that we are interested in. For example, suppose as economists we are interested in the relationship between the top marginal tax rate and the unemployment rate. Pundits and politicians often claim that wealthy people are so-called ``job creators'' and that raising the top marginal tax rate will cause an increase in unemployment. Suppose that their evidence is that when the top marginal tax rate in the United States was higher, unemployment was higher as well. In the United States in 1965, the top federal income tax rate was for every dollar earned over \$1.6 million was 70\%.\footnote{This amount has been adjusted for inflation; the unadjusted amount was \$200,000.} In 2015, the top federal income tax rate for every dollar earned over approximately \$400,000 was 39.6\%. Meanwhile, according to the U.S. Bureau of Labor Statistics the unemployment rate in 1965 was 4\% while in 2015 it was 5\%. Clearly, there is no association between the top marginal tax rate and the unemployment rate. This means that these two rates are \textsc{\gls{independent}}. When two properties or variables are independent, knowing the value of one of them is not informative about the value of the other.

\newglossaryentry{independent}
{
name=Independent,
description={Two properties or variables are independent if there is no association between them. That is, $X$ and $Y$ are independent if $pr(X|Y)=pr(X)$.}
}

Suppose, however, that there are other reasons to think the two features are associated. An association between two features X and Y can be explained in one of four possible ways.

\begin{enumerate}
\item X causes Y.
\item Y causes X.
\item X and Y are both caused by a common cause, Z.
\item X and Y are coincidentally associated.
\end{enumerate}

There are many associations that are merely coincidentally associated. You can see some of these associations here: \href{https://www.tylervigen.com/spurious-correlations}{link}.

In~\ref{part:causation} we'll discuss causation and causal modeling in more detail. However, to understand the formalization of causation that has been happening in philosophy and machine learning in the last few decades we'll first need the underlying probability theory. That means we'll need to formalize our inductive logic.







\section*{Key Terms}
\begin{fullwidth}
\begin{sortedlist}
\sortitem{Hypothesis}{}
\sortitem{Hypothetico-deductive method}{}
\sortitem{Necessary condition}{}
\sortitem{Sufficient condition}{}
\sortitem{Process of confirmation}{}
\sortitem{Process of disconfirmation}{}
\sortitem{Pseudoscience}{}
\sortitem{Independence}{}
\sortitem{Causation}{}
\end{sortedlist}
\end{fullwidth}
